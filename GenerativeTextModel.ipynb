{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c6c2ec-f756-4044-9b83-0132f18fe3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Error [WinError 2] The system cannot find the file specified while executing command git version\n",
      "ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
     ]
    }
   ],
   "source": [
    "# Install Gradio for creating a simple web interface\n",
    "# Install the latest version of Hugging Face Transformers (for GPT-2)\n",
    "!pip install -q gradio\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109bd78-79b4-4a5d-b3dc-9f5776cc0f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2463573-edfd-4abc-806c-16f2e364823d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gmkai\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Gradio for UI, TensorFlow backend for running GPT2, and the tokenizer/model from Transformers\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d12f7432-e06c-4d33-a9ab-890dcc2313f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f84d18fb99341e0b3e5933c94832d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gmkai\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gmkai\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gmkai\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the GPT-2 tokenizer to convert input text into tokens\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the GPT-2 model and assign the end-of-sequence token for padding\n",
    "# This model will be used to generate text\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    pad_token_id=tokenizer.eos_token_id  # Ensures the model handles padding correctly\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804866a-3fa9-4234-8ec6-d0daa1c2f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts user input and generates a text paragraph\n",
    "def generate_text(input_text):\n",
    "    # Convert the input text into model-readable format (tensor of tokens)\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
    "\n",
    "    # Generate new text using beam search (more coherent than greedy or random)\n",
    "    beam_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=350,              # Maximum length of the generated text\n",
    "        num_beams=5,                 # Beam search with 5 beams (better quality)\n",
    "        no_repeat_ngram_size=2,     # Avoid repeating 2-word phrases\n",
    "        early_stopping=True         # Stop when output seems complete\n",
    "    )\n",
    "\n",
    "    # Decode the generated output into human-readable text\n",
    "    output = tokenizer.decode(\n",
    "        beam_output[0],\n",
    "        skip_special_tokens=True,   # Remove tokens like\n",
    "        clean_up_tokenization_spaces=True  # Clean extra spaces\n",
    "    )\n",
    "\n",
    "    # Return only full sentences, cleaned up\n",
    "    return \".\".join(output.split(\".\")) + \".\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016f1ef6-af68-4f90-8ad3-704f764379a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m output_text \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mTextbox()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create and launch the Gradio interface\u001b[39;00m\n\u001b[0;32m      5\u001b[0m gr\u001b[38;5;241m.\u001b[39mInterface(\n\u001b[1;32m----> 6\u001b[0m     fn\u001b[38;5;241m=\u001b[39m\u001b[43mgenerate_text\u001b[49m,         \u001b[38;5;66;03m# Function to run on input\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextbox\u001b[39m\u001b[38;5;124m\"\u001b[39m,         \u001b[38;5;66;03m# Input component (user prompt)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     outputs\u001b[38;5;241m=\u001b[39moutput_text,      \u001b[38;5;66;03m# Output component (generated paragraph)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ§  GENERATIVE TEXT MODEL (GPT-2)\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Interface title\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\u001b[38;5;241m.\u001b[39mlaunch()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_text' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a textbox output component\n",
    "output_text = gr.Textbox()\n",
    "\n",
    "# Create and launch the Gradio interface\n",
    "gr.Interface(\n",
    "    fn=generate_text,         # Function to run on input\n",
    "    inputs=\"textbox\",         # Input component (user prompt)\n",
    "    outputs=output_text,      # Output component (generated paragraph)\n",
    "    title=\"ðŸ§  GENERATIVE TEXT MODEL (GPT-2)\"  # Interface title\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31bf260-3d67-40a6-91dd-39875491a660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
